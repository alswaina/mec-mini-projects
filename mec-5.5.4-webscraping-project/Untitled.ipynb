{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d980169d",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Scrapy\n",
      "  Downloading Scrapy-2.8.0-py2.py3-none-any.whl (272 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.9/272.9 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting itemadapter>=0.1.0\n",
      "  Downloading itemadapter-0.8.0-py3-none-any.whl (11 kB)\n",
      "Collecting PyDispatcher>=2.0.5\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Collecting w3lib>=1.17.0\n",
      "  Downloading w3lib-2.1.1-py3-none-any.whl (21 kB)\n",
      "Collecting protego>=0.1.15\n",
      "  Downloading Protego-0.2.1-py2.py3-none-any.whl (8.2 kB)\n",
      "Collecting parsel>=1.5.0\n",
      "  Downloading parsel-1.8.1-py2.py3-none-any.whl (17 kB)\n",
      "Collecting tldextract\n",
      "  Downloading tldextract-3.4.1-py3-none-any.whl (92 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.9/92.9 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting zope.interface>=5.1.0\n",
      "  Downloading zope.interface-6.0-cp310-cp310-macosx_10_9_x86_64.whl (202 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.3/202.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting service-identity>=18.1.0\n",
      "  Downloading service_identity-21.1.0-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: packaging in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from Scrapy) (23.1)\n",
      "Collecting cssselect>=0.9.1\n",
      "  Downloading cssselect-1.2.0-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: setuptools in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from Scrapy) (66.0.0)\n",
      "Collecting pyOpenSSL>=21.0.0\n",
      "  Downloading pyOpenSSL-23.1.1-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.9/57.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting queuelib>=1.4.2\n",
      "  Downloading queuelib-1.6.2-py2.py3-none-any.whl (13 kB)\n",
      "Collecting lxml>=4.3.0\n",
      "  Downloading lxml-4.9.2-cp310-cp310-macosx_10_15_x86_64.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting cryptography>=3.4.6\n",
      "  Downloading cryptography-40.0.2-cp36-abi3-macosx_10_12_x86_64.whl (2.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Twisted>=18.9.0\n",
      "  Downloading Twisted-22.10.0-py3-none-any.whl (3.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting itemloaders>=1.0.1\n",
      "  Downloading itemloaders-1.1.0-py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from cryptography>=3.4.6->Scrapy) (1.15.1)\n",
      "Collecting jmespath>=0.9.5\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Requirement already satisfied: six in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from protego>=0.1.15->Scrapy) (1.16.0)\n",
      "Collecting pyasn1-modules\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=19.1.0 in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from service-identity>=18.1.0->Scrapy) (23.1.0)\n",
      "Collecting pyasn1\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting hyperlink>=17.1.1\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.6/74.6 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Automat>=0.8.0\n",
      "  Downloading Automat-22.10.0-py2.py3-none-any.whl (26 kB)\n",
      "Collecting typing-extensions>=3.6.5\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting constantly>=15.1\n",
      "  Downloading constantly-15.1.0-py2.py3-none-any.whl (7.9 kB)\n",
      "Collecting incremental>=21.3.0\n",
      "  Downloading incremental-22.10.0-py2.py3-none-any.whl (16 kB)\n",
      "Collecting requests>=2.1.0\n",
      "  Downloading requests-2.29.0-py3-none-any.whl (62 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting requests-file>=1.4\n",
      "  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\n",
      "Requirement already satisfied: idna in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from tldextract->Scrapy) (3.4)\n",
      "Collecting filelock>=3.0.8\n",
      "  Downloading filelock-3.12.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pycparser in /Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4.6->Scrapy) (2.21)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-macosx_10_9_x86_64.whl (124 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.8/124.8 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting certifi>=2017.4.17\n",
      "  Downloading certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.3/155.3 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.9/140.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: PyDispatcher, incremental, constantly, zope.interface, w3lib, urllib3, typing-extensions, queuelib, pyasn1, protego, lxml, jmespath, itemadapter, hyperlink, filelock, cssselect, charset-normalizer, certifi, Automat, Twisted, requests, pyasn1-modules, parsel, cryptography, service-identity, requests-file, pyOpenSSL, itemloaders, tldextract, Scrapy\n",
      "Successfully installed Automat-22.10.0 PyDispatcher-2.0.7 Scrapy-2.8.0 Twisted-22.10.0 certifi-2022.12.7 charset-normalizer-3.1.0 constantly-15.1.0 cryptography-40.0.2 cssselect-1.2.0 filelock-3.12.0 hyperlink-21.0.0 incremental-22.10.0 itemadapter-0.8.0 itemloaders-1.1.0 jmespath-1.0.1 lxml-4.9.2 parsel-1.8.1 protego-0.2.1 pyOpenSSL-23.1.1 pyasn1-0.5.0 pyasn1-modules-0.3.0 queuelib-1.6.2 requests-2.29.0 requests-file-1.5.1 service-identity-21.1.0 tldextract-3.4.1 typing-extensions-4.5.0 urllib3-1.26.15 w3lib-2.1.1 zope.interface-6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# install Scrapy\n",
    "#pip install Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "177c5b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://quotes.toscrape.com/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbfdaa4",
   "metadata": {},
   "source": [
    "1. Creating a new Scrapy project\n",
    "2. Writing a spider to crawl a site and extract data\n",
    "3. Exporting the scraped data using the command line\n",
    "4. Changing spider to recursively follow links\n",
    "5. Using spider arguments\n",
    "6. Load the scraped data into a SQLlite3 database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d477b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'my_scrapy_mini_project', using template directory '/Users/fahad/opt/anaconda3/envs/mec-554/lib/python3.10/site-packages/scrapy/templates/project', created in:\r\n",
      "    /Users/fahad/Documents/MLE_COURSE/MLE_PROJECTS/mec-mini-projects/mec-5.5.4-webscraping-project/my_scrapy_mini_project\r\n",
      "\r\n",
      "You can start your first spider with:\r\n",
      "    cd my_scrapy_mini_project\r\n",
      "    scrapy genspider example example.com\r\n"
     ]
    }
   ],
   "source": [
    "#1. Creating a new Scrapy project\n",
    "!scrapy startproject my_scrapy_mini_project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf8a5e38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34m.\u001b[00m\r\n",
      "├── \u001b[01;34mmy_scrapy_mini_project\u001b[00m\r\n",
      "│   ├── __init__.py\r\n",
      "│   ├── items.py\r\n",
      "│   ├── middlewares.py\r\n",
      "│   ├── pipelines.py\r\n",
      "│   ├── settings.py\r\n",
      "│   └── \u001b[01;34mspiders\u001b[00m\r\n",
      "│       └── __init__.py\r\n",
      "└── scrapy.cfg\r\n",
      "\r\n",
      "2 directories, 7 files\r\n"
     ]
    }
   ],
   "source": [
    "cd my_scrapy_mini_project/\n",
    "!tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e176a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untitled.ipynb          \u001b[34mmy_scrapy_mini_project\u001b[m\u001b[m  \u001b[31mscrapy-mini-project.rst\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "#2. Writing a spider to crawl a site and extract data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a3627ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Exporting the scraped data using the command line\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b42843ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4. Changing spider to recursively follow links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2553bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Using spider arguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d3bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6. Load the scraped data into a SQLlite3 database\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
